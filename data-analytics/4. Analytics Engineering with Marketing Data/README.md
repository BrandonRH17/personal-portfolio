# Analytics Engineering with Marketing Data
### Campaign Performance Pipeline with dbt & DuckDB

---

## ğŸ“‹ Project Overview (STAR Format)

### ğŸ¯ Situation

Marketing teams generate large volumes of data across campaigns, ad clicks, and conversions, but raw data often arrives with inconsistent formatting, mixed casing, and unvalidated fields. Without a structured transformation layer, deriving reliable campaign performance metrics becomes error-prone and difficult to maintain.

This project addresses that challenge by building a **modern analytics engineering pipeline** using dbt and DuckDB â€” demonstrating how raw marketing data can be systematically cleaned, tested, and transformed into business-ready insights.

### ğŸ“ Task

Design and implement an end-to-end data transformation pipeline that:

- Ingests synthetic but realistic marketing data (50K customers, 200 campaigns, 300K clicks, 60K conversions)
- Cleans and standardizes raw data through a staging layer
- Aggregates key campaign performance metrics in a mart layer
- Validates data quality through comprehensive testing at every layer

### âš™ï¸ Action

**Technologies & Architecture:**

- **dbt-core**: SQL transformation framework following software engineering best practices
- **DuckDB**: Embedded analytical database â€” zero infrastructure, runs locally
- **Python + Faker**: Reproducible synthetic data generation (seed = 42)
- **Jupyter Notebook**: Raw data exploration and transformation design

**Pipeline Architecture:**

```
Raw Data (Python/Faker)  â†’  DuckDB  â†’  dbt Staging (views)  â†’  dbt Mart (tables)
                           raw schema   staging schema         marts schema
```

**Key Implementation Details:**

1. **Staging Layer** â€” 4 models that clean each source table:
   - Email validation and lowercasing via custom `clean_email` macro
   - Name and country standardization via `proper_case` macro
   - Type casting (timestamps â†’ dates) and rounding (USD to 2 decimals)

2. **Mart Layer** â€” Campaign performance aggregation:
   - Total clicks, unique clicked users, average cost per click
   - Converted users, cost per conversion, total revenue generated
   - Joins across campaigns, clicks, and conversions

3. **Testing Strategy** â€” Generic tests (unique, not_null, accepted_values) + custom singular tests for business rules (non-negative costs, valid conversion counts)

### ğŸ¯ Result

âœ… **Fully Reproducible Pipeline** â€” Anyone can clone, generate data, and run the entire pipeline in under 2 minutes with 6 commands

âœ… **Data Quality Assurance** â€” 20+ automated tests validating uniqueness, nullability, accepted values, and business logic

âœ… **Clean Architecture** â€” Follows dbt best practices: source definitions, YAML documentation, staging/mart separation, reusable macros

âœ… **Zero Infrastructure** â€” Runs entirely on DuckDB (no cloud services, no database server needed)

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/          # 4 cleaning models + YAML docs
â”‚   â”‚   â””â”€â”€ mart/             # Campaign performance aggregation
â”‚   â”œâ”€â”€ macros/               # clean_email, proper_case
â”‚   â”œâ”€â”€ tests/                # Custom singular tests
â”‚   â”œâ”€â”€ profiles/             # DuckDB connection config
â”‚   â”œâ”€â”€ generate_data.py      # Synthetic data generator
â”‚   â”œâ”€â”€ exploration.ipynb     # Raw data exploration
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md             # Technical setup guide
â””â”€â”€ README.md                 # This file
```

---

## ğŸš€ Quick Start

```bash
cd code
python -m venv venv && venv\Scripts\activate   # Windows
pip install -r requirements.txt
python generate_data.py
dbt run --profiles-dir profiles
dbt test --profiles-dir profiles
```

See [code/README.md](code/README.md) for detailed setup instructions.

---
