{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1a13b4-f75a-4296-b0a0-ec0d9fcf3baa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "# GDELT Country Exploration: Maritime Port Disruption Analysis\n\n## Project Overview\n\nThis notebook demonstrates a comprehensive data analytics approach to predict maritime port disruptions using the GDELT (Global Database of Events, Language, and Tone) dataset. The analysis focuses on the **Transpacific Route** and key ports across Canada, USA, China, and Japan.\n\n### Key Objectives:\n1. **Data Validation & Cleaning**: Validate and clean raw GDELT data to ensure quality and reliability\n2. **Theme Identification**: Identify port-related news with relevant themes (infrastructure, trade, incidents)\n3. **Sentiment Analysis**: Filter emotionally-charged news to focus on factual reporting\n4. **Predictive Analytics**: Analyze news patterns and growth to predict potential disruptions\n\n### Workflow:\n```\nRaw Data → Cleaning & Validation → Theme Extraction → Sentiment Filtering → Pattern Analysis → Predictions\n```\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e62a01f-0cfb-4938-b65a-e30b6cc557fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 1. Setup: Libraries and Dependencies\n\nImport necessary libraries for data processing, transformation, and analysis using PySpark SQL functions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8a3acf-a22c-4856-adef-e17c5f03ffcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# LIBRARY IMPORTS\n# ============================================\n# Core PySpark libraries for distributed data processing\n\nfrom pyspark.sql.functions import *  # SQL functions for data transformation\nimport datetime                       # Date/time operations\nfrom pyspark.sql.window import Window # Window functions for time-series analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e145e8aa-7ea5-42b3-bbe7-40beb9a25d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 2. User-Defined Functions (UDFs)\n\nCustom functions created to support geospatial calculations and data transformations throughout the analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5320dc70-367a-4da9-b35d-c8d543e2a7a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# UDF: GEOSPATIAL DISTANCE CALCULATION\n# ============================================\n\ndef cal_lat_log_dist(df, lat1, long1, lat2, long2):\n    \"\"\"\n    Calculate the great-circle distance between two geographic points using the Haversine formula.\n    \n    This function computes the shortest distance over the earth's surface between two points\n    specified by their latitude and longitude coordinates.\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input PySpark DataFrame to which the distance column will be added\n    lat1 : str\n        Column name containing latitude of the first point (in degrees)\n    long1 : str\n        Column name containing longitude of the first point (in degrees)\n    lat2 : str\n        Column name containing latitude of the second point (in degrees)\n    long2 : str\n        Column name containing longitude of the second point (in degrees)\n    \n    Returns:\n    --------\n    DataFrame\n        Original DataFrame with an additional 'distance_in_kms' column containing\n        the calculated distance in kilometers, rounded to 4 decimal places\n    \n    Formula:\n    --------\n    Uses the Haversine formula:\n    distance = R × arccos(sin(lat1) × sin(lat2) + cos(lat1) × cos(lat2) × cos(long1 - long2))\n    where R = 6371 km (Earth's radius)\n    \n    Example:\n    --------\n    >>> df_with_distance = cal_lat_log_dist(df, 'port_lat', 'port_long', 'event_lat', 'event_long')\n    \"\"\"\n    \n    # Haversine formula implementation using PySpark SQL functions\n    df = df.withColumn('distance_in_kms',\n        round(\n            (acos(\n                (sin(radians(col(lat1))) * sin(radians(col(lat2)))) +\n                ((cos(radians(col(lat1))) * cos(radians(col(lat2)))) *\n                 (cos(radians(long1) - radians(long2))))\n            ) * lit(6371.0)),  # Earth's radius in kilometers\n            4  # Round to 4 decimal places for precision\n        )\n    )\n    return df"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e75d4a-5434-4f62-a502-793b4cbf54a8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 3. Data Loading from Bronze Layer\n\n### Medallion Architecture - Bronze Layer\nThe Bronze layer contains raw, unprocessed data from source systems. This follows the **Medallion Architecture** best practice for data lakes:\n- **Bronze**: Raw ingestion layer (current layer)\n- **Silver**: Cleaned and validated data\n- **Gold**: Business-level aggregates\n\n### Data Sources:\n\n1. **GDELT_EVENTS**: Core GDELT events table containing global news events with actors, locations, and event codes\n2. **GDELT_GKG** (Global Knowledge Graph): Enhanced metadata including themes, tone, and sentiment analysis\n3. **PORTS_DICTIONARY**: Reference table with global port locations and coordinates\n4. **CAMEO_DICTIONARY**: CAMEO (Conflict and Mediation Event Observations) event code descriptions for event classification\n\nAll tables are loaded directly from the Bronze database in their raw form for initial exploration and validation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66894ba2-d9ad-4e16-8b4e-63780e69e2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# LOAD RAW DATA FROM BRONZE LAYER\n# ============================================\n\n# Load GDELT events table - contains structured event data\nGDELT_EVENTS = spark.sql(\"SELECT * FROM BRONZE.GDELT_EVENTS\")\n\n# Load port locations reference data - contains coordinates and metadata for ports worldwide\nPORT_LOCATIONS_DIM = spark.sql(\"SELECT * FROM BRONZE.PORTS_DICTIONARY\")\n\n# Load CAMEO event code dictionary - maps event codes to human-readable descriptions\nCAMEO_DICTIONARY = spark.sql(\"SELECT * FROM BRONZE.CAMEO_DICTIONARY\")\n\n# Load GDELT Global Knowledge Graph - contains themes, tones, and enriched metadata\nGKG = spark.sql(\"SELECT * FROM BRONZE.GDELT_GKG\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c516bf38-65b6-43c3-9a77-cf754781b890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 4. Data Quality: Cleaning PORT_LOCATIONS_DIM\n\n### Data Quality Issues Identified:\nThe raw port locations data requires cleaning due to several data quality issues:\n- ❌ Missing coordinate values (NULL latitudes/longitudes)\n- ❌ Inconsistent formatting with embedded spaces\n- ❌ Directional indicators (N, S, E, W) included in numeric values\n- ❌ Lack of standardized decimal coordinate format\n\n### Cleaning Process:\n\n1. **Filter Missing Values**: Remove records with NULL coordinates\n2. **Standardize Format**: Remove whitespace from coordinate strings\n3. **Extract Orientation**: Parse directional indicators (N/S/E/W)\n4. **Convert Coordinates**: Transform to decimal degrees with proper signs:\n   - North (N) and East (E) → Positive values\n   - South (S) and West (W) → Negative values\n5. **Select Clean Columns**: Output only validated, standardized fields\n\n### Output Schema:\n| Column | Type | Description |\n|--------|------|-------------|\n| COUNTRY | String | Country name |\n| PORT | String | Port name |\n| LATITUDE_CORRECTED | Float | Decimal latitude (-90 to 90) |\n| LONGITUDE_CORRECTED | Float | Decimal longitude (-180 to 180) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10c16be-af27-4424-9fa6-5e2d4f0f2eeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# PORT LOCATIONS DATA CLEANING PIPELINE\n# ============================================\n\nPORT_LOCATIONS_DIM_CLEANED = (\n    PORT_LOCATIONS_DIM\n    \n    # STEP 1: Filter out records with missing coordinate data\n    .filter(\"LATITUDE IS NOT NULL\")   # Ensure latitude exists\n    .filter(\"LONGITUDE IS NOT NULL\")  # Ensure longitude exists\n    \n    # STEP 2: Standardize format - remove embedded whitespace\n    .withColumn(\"LATITUDE\", regexp_replace(col(\"LATITUDE\"), \" \", \"\"))   # Clean latitude string\n    .withColumn(\"LONGITUDE\", regexp_replace(col(\"LONGITUDE\"), \" \", \"\")) # Clean longitude string\n    \n    # STEP 3: Extract directional orientation from coordinate strings\n    # Format example: \"37.7749N\" → Extract \"N\"\n    .withColumn(\"Lat_Ori\", substring(col(\"LATITUDE\"), -1, 1))   # Get last char (N/S)\n    .withColumn(\"Long_Ori\", substring(col(\"LONGITUDE\"), -1, 1)) # Get last char (E/W)\n    \n    # STEP 4: Convert to decimal degrees with correct signs\n    # Apply conversion logic based on directional indicators\n    .withColumn(\"LATITUDE_CORRECTED\",\n        when(col(\"Lat_Ori\") == 'S',  # South → Negative\n             expr(\"substring(LATITUDE, 1, length(LATITUDE) - 1)\") * -1)\n        .when(col(\"Lat_Ori\") == 'N',  # North → Positive\n              expr(\"substring(LATITUDE, 1, length(LATITUDE) - 1)\"))\n        .when(col(\"Lat_Ori\") == 'E',  # Edge case: E marked as lat\n              expr(\"substring(LATITUDE, 1, length(LATITUDE) - 1)\") * -1)\n        .otherwise(999.999)  # Flag for unmapped values (data quality check)\n    )\n    .withColumn(\"LONGITUDE_CORRECTED\",\n        when(col(\"Long_Ori\") == 'E',  # East → Positive\n             expr(\"substring(LONGITUDE, 1, length(LONGITUDE) - 1)\"))\n        .when(col(\"Long_Ori\") == 'W',  # West → Negative\n              expr(\"substring(LONGITUDE, 1, length(LONGITUDE) - 1)\") * -1)\n        .when(col(\"Lat_Ori\") == 'N',  # Edge case: N marked as long\n              expr(\"substring(LATITUDE, 1, length(LATITUDE) - 1)\") * -1)\n        .otherwise(999.999)  # Flag for unmapped values (data quality check)\n    )\n    \n    # STEP 5: Select only clean, relevant columns for downstream processing\n    .select(\"COUNTRY\", \"PORT\", \"LATITUDE_CORRECTED\", \"LONGITUDE_CORRECTED\")\n)\n\n# Display sample of cleaned data for validation\n# PORT_LOCATIONS_DIM_CLEANED.display()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fceb68ec-e4b9-4afd-a041-c27f46cb6777",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## OBJECTIVE - DATA ANALYSIS:\n",
    "\n",
    "To start doing some analysis with our data. We have to do a clear explanaition of our objective. Our Objective is:\n",
    "\n",
    "**Detect and Precidit possible complications in port related to the Transpacific Route**\n",
    "\n",
    "First, we need to identify the news that could be related to Ports inside the Transpacific Route. The countries we select to analyze are: Canada, United States, China and Japan. and for each country we select the three more importants ports that are useden in the Transpacific Route. The following list is a compilation per country that discribes: \n",
    "* Name of Country\n",
    "* Code for Country\n",
    "* Name of Port\n",
    "* Location of the Port\n",
    "* Code of the Location of the Port\n",
    "\n",
    "**For further reference in locations you can check the following summary**\n",
    "\n",
    "**CANADA**\n",
    "Code for country = 'CA'\n",
    "\n",
    "1. Port of Vancouver - British Columbia (CA02)\n",
    "2. Puerto de Prince Rupert - Columbia Británica (CA02)\n",
    "3. Port of Montreal - Quebec (CA10)\n",
    "\n",
    "**USA**\n",
    "Code for country = 'US'\n",
    "\n",
    "1. Port of Los Angeles - California (USCA)\n",
    "2. Port of Long Beach - California (USCA)\n",
    "3. Port of Oakland - California (USCA)\n",
    "\n",
    "**CHINA**\n",
    "Code for country = 'CH'\n",
    "\n",
    "1. Port of Shanghai - Shanghai (CH23)\n",
    "2. Port of Shenzhen - Guangdong Province (CH30)\n",
    "3. Port of Ningbo-Zhoushan - Zhejiang Province (CH02)\n",
    "\n",
    "**JAPAN**\n",
    "Code for Country = 'JA'\n",
    "\n",
    "1. Port of Tokyo - Tokyo (JA40)\n",
    "2. Port of Yokohama - Kanagawa Prefecture (JA19)\n",
    "3. Port of Nagoya - Aichi Prefecture (JA01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e195a210-3d2b-40c5-8c0e-aba53858ee85",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 5. GKG Data Cleaning & Preparation\n\n### Why Data Cleaning is Critical:\nThe GDELT GKG (Global Knowledge Graph) table contains complex, semi-structured data that requires extensive cleaning before analysis. Raw GKG data presents several challenges:\n- **Inconsistent date formats**: Need standardization for time-series analysis\n- **Delimited fields**: LOCATIONS and TONE columns contain multiple values separated by delimiters\n- **Large volume**: Filtering by time period reduces computational overhead\n- **Geographic specificity**: Need to extract country and location codes for filtering\n\n### Cleaning Operations:\n\n| Step | Operation | Purpose |\n|------|-----------|---------|\n| 1 | Date Standardization | Convert YYYYMMDD string → proper Date type |\n| 2 | Time Period Filter | Limit to 2022-01-01 to 2024-07-31 for relevant analysis |\n| 3 | Location Parsing | Split LOCATIONS field (delimited by '#') to extract codes |\n| 4 | Tone Decomposition | Split TONE field (delimited by ',') into sentiment components |\n| 5 | Geographic Filter | Keep only news from Transpacific Route port locations |\n\n### Extracted Sentiment Metrics:\n- **AverageTone**: Overall sentiment score (-100 to +100, negative = bad news)\n- **TonePositiveScore**: Percentage of positive words in the document\n- **ToneNegativeScore**: Percentage of negative words in the document\n- **Polarity**: Spread between positive and negative (higher = more extreme)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5ac314-595a-483d-9b97-bdd215a4db52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# GKG PRIMARY DATA CLEANING PIPELINE\n# ============================================\n\nGKG_PRINCIPAL_CLEANING = (\n    GKG\n    \n    # STEP 1: Standardize date format for time-series analysis\n    .withColumn(\"Date\", to_date(col(\"DATE\"), \"yyyyMMdd\"))  # Convert YYYYMMDD string to Date type\n    \n    # STEP 2: Filter to relevant time period (Jan 2022 - July 2024)\n    # This reduces data volume and focuses on recent, relevant events\n    .filter(\"Date >= '2022-01-01' and Date < '2024-08-01'\")\n    \n    # STEP 3: Parse location information from delimited LOCATIONS field\n    # Format: \"type#typecode#countrycode#locationcode#...\"\n    .withColumn(\"CountryCode\", split(col(\"LOCATIONS\"), \"#\").getItem(2))   # Extract country code (index 2)\n    .withColumn(\"LocationCode\", split(col(\"LOCATIONS\"), \"#\").getItem(3))  # Extract location code (index 3)\n    \n    # STEP 4: Decompose TONE field into sentiment components\n    # Format: \"averageTone,positiveScore,negativeScore,polarity,...\"\n    .withColumn(\"AverageTone\", split(col(\"TONE\"), \",\").getItem(0))        # Overall sentiment (-100 to +100)\n    .withColumn(\"TonePositiveScore\", split(col(\"TONE\"), \",\").getItem(1))  # % positive words\n    .withColumn(\"ToneNegativeScore\", split(col(\"TONE\"), \",\").getItem(2))  # % negative words\n    .withColumn(\"Polarity\", split(col(\"TONE\"), \",\").getItem(3))           # Emotional spread\n    \n    # STEP 5: Filter to Transpacific Route port locations only\n    # Location codes correspond to major ports in Canada, USA, China, and Japan\n    # See \"Objective - Data Analysis\" section for detailed location code mappings\n    .filter(col(\"LocationCode\").isin(\n        'CA02',  # British Columbia, Canada (Vancouver, Prince Rupert)\n        'CA10',  # Quebec, Canada (Montreal)\n        'USCA',  # California, USA (Los Angeles, Long Beach, Oakland)\n        'CH23',  # Shanghai, China\n        'CH30',  # Guangdong, China (Shenzhen)\n        'CH02',  # Zhejiang, China (Ningbo-Zhoushan)\n        'JA40',  # Tokyo, Japan\n        'JA19',  # Kanagawa, Japan (Yokohama)\n        'JA01'   # Aichi, Japan (Nagoya)\n    ))\n)\n\n# The cleaned dataset is now ready for sentiment filtering and theme analysis"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5be312-5137-40a6-a987-5cbd6b7ad97d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TABLE_OF_DATES = (\n",
    "GKG_PRINCIPAL_CLEANING\n",
    ".select(\"Date\").distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c4a655-eb1c-4392-8a64-f97f1023b024",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## 6. Filtering Emotionally-Charged News\n\n### The Problem: Emotional Bias in News Reporting\n\nAs a logistics company making critical operational decisions, we cannot rely on emotionally-charged reporting that may distort the factual situation. Highly emotional news can lead to:\n- **Misinterpretation** of actual risk levels\n- **Overreaction** to minor incidents\n- **Poor decision-making** based on sensationalism rather than facts\n\n### GDELT's Emotional Charge Identification\n\nAccording to the GDELT documentation, emotionally-charged news exhibits a specific pattern:\n- **Neutral average tone** (close to 0) → Not clearly positive or negative\n- **High polarity** (large spread) → Contains both very positive and very negative language\n\nThis combination indicates reporting that uses extreme language from both perspectives, suggesting emotional manipulation rather than objective journalism.\n\n### Our Filtering Criteria:\n\n| Metric | Threshold | Rationale |\n|--------|-----------|-----------|\n| **Neutrality Range** | -0.5 to +0.5 | Tone is neither clearly positive nor negative |\n| **High Polarity** | ≥ 9.0 | Above 85th percentile based on data distribution |\n\n**Filtering Logic:**\n```\nEmotionally Charged = (AverageTone between -0.5 and 0.5) AND (Polarity ≥ 9)\nKeep Only: News WHERE Emotionally Charged = False\n```\n\n### Why This Matters:\nBy filtering out emotional content, we ensure our disruption predictions are based on **factual reporting** of actual incidents rather than sensationalized coverage."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c227c4-bf10-4a13-aade-184b941dcbda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# FILTER EMOTIONALLY-CHARGED NEWS\n# ============================================\n\nGKG_NOT_EMOTIONAL_CHARGE = (\n    GKG_PRINCIPAL_CLEANING\n    \n    # STEP 1: Identify neutral tone news\n    # News with AverageTone close to 0 (-0.5 to +0.5) are considered neutral\n    .withColumn(\"Neutrality\",\n        when((col(\"AverageTone\") >= -0.5) & (col(\"AverageTone\") <= 0.5), 1)  # Flag as neutral (1)\n        .otherwise(0)  # Not neutral (0)\n    )\n    \n    # STEP 2: Flag emotionally-charged content\n    # Combination of neutral tone + high polarity = emotional manipulation\n    .withColumn(\"EC\",  # EC = Emotional Charge flag\n        when(\n            (col(\"Neutrality\") == 1) &      # Tone is neutral, BUT...\n            (col('Polarity') >= 9),         # ...polarity is extreme (≥85th percentile)\n            1  # Flag as emotionally charged\n        )\n        .otherwise(0)  # Not emotionally charged\n    )\n    \n    # STEP 3: Filter to keep only factual, non-emotional news\n    # We want EC == 0 (not emotionally charged) for objective analysis\n    .filter(\"EC == 0\")\n)\n\n# Result: Dataset now contains only news with objective, factual reporting\n# This ensures our disruption predictions are based on real events, not sensationalism"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c09a5f-03cf-459c-942c-d67a83b591f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "---\n\n## 7. Approach 1: Theme-Based News Growth Analysis\n\n### Objective:\nPredict port disruptions by analyzing **growth patterns** in negative news coverage across multiple relevant themes.\n\n### Hypothesis:\nAn **increase in negative news** related to port activities and associated themes (infrastructure, trade, incidents) may signal impending disruptions before they become critical.\n\n### Theme Selection Strategy:\n\nWe identify port-related news through a **base filter** plus **key theme combinations**:\n\n| Theme | GDELT Code | Why It Matters |\n|-------|------------|----------------|\n| **Base Filters** | | |\n| PORT | `THEMES LIKE '%PORT%'` | Direct port mentions |\n| TRANSPORT | `THEMES LIKE '%TRANSPORT%'` | Shipping and logistics |\n| NOT AIRPORT | `THEMES NOT LIKE '%AIRPORT%'` | Exclude air transport |\n| | | |\n| **Key Themes** | | |\n| Transport Infrastructure | `TRANSPORT_INFRASTRUCTURE` | Port facility issues, construction |\n| Trade | `TRADE` | Trade disputes, tariffs, policy changes |\n| Macroeconomic | `MACROECONOMIC` | Economic factors affecting ports |\n| Public Sector | `PUBLIC_SECTOR` | Government regulations, strikes |\n| Maritime Incident | `MARITIME_INCIDENT` | Accidents, blockages, safety issues |\n\n### Analysis Method:\n1. **Identify** news matching base criteria (PORT + TRANSPORT - AIRPORT)\n2. **Tag** each news article with applicable themes (1 if present, 0 if not)\n3. **Count** total theme occurrences per article\n4. **Filter** to negative sentiment only (AverageTone < 0)\n5. **Aggregate** daily counts by location and theme combination\n6. **Analyze growth** patterns over time using window functions"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82301d5b-e94f-4270-a876-e4d458cdc947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de74ab5-3f71-4e73-ac9b-7f93855d5291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b245c75-a6f9-4222-b8c8-83c1cac5d3be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d60619-287a-479e-af98-8f8b9f5d265c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# APPROACH 1: THEME-BASED NEWS GROWTH ANALYSIS\n# ============================================\n\n# PART 1: Theme identification and aggregation\nGKG_PORTS_OV_FIRST_APPROACH = (        \n    GKG_NOT_EMOTIONAL_CHARGE\n    \n    # STEP 1: Apply base filters to identify port-related news\n    .withColumn(\"BaseNews\",\n        when(\n            (col(\"THEMES\").like(\"%PORT%\")) &          # Must mention ports\n            (col(\"THEMES\").like(\"%TRANSPORT%\")) &     # Must mention transport\n            (~col(\"THEMES\").like(\"%AIRPORT%\")),       # Exclude airports\n            1  # Flag as base port news\n        )\n    )\n    .filter(\"BaseNews == 1\")  # Keep only port-related news\n    \n    # STEP 2: Tag news with relevant theme flags (1 if present, 0 if not)\n    .withColumn(\"NewsWithTINFA\",  # Transport Infrastructure\n        when(col(\"THEMES\").like(\"%TRANSPORT_INFRASTRUCTURE%\"), 1))\n    .withColumn(\"NewsWithTRADE\",  # Trade-related\n        when(col(\"THEMES\").like(\"%TRADE%\"), 1))\n    .withColumn(\"NewsWithME\",     # Macroeconomic\n        when(col(\"THEMES\").like(\"%MACROECONOMIC%\"), 1))\n    .withColumn(\"NewsWithPS\",     # Public Sector\n        when(col(\"THEMES\").like(\"%PUBLIC_SECTOR%\"), 1))\n    .withColumn(\"NewsWithMI\",     # Maritime Incident\n        when(col(\"THEMES\").like(\"%MARITIME_INCIDENT%\"), 1))\n    .fillna(0)  # Fill NULL flags with 0\n    \n    # STEP 3: Calculate total number of themes present in each news article\n    .withColumn(\"Total\",\n        col(\"NewsWithTINFA\") + col(\"NewsWithTRADE\") + \n        col(\"NewsWithME\") + col(\"NewsWithPS\") + col(\"NewsWithMI\")\n    )\n    \n    # STEP 4: Filter to negative news only (potential disruption signals)\n    .filter(\"AverageTone < 0\")  # Only keep news with negative sentiment\n    \n    # STEP 5: Aggregate by date, theme combination, and location\n    .groupby(\"Date\", \"Total\", \"LocationCode\")\n    .agg(\n        sum(\"NewsWithTINFA\").alias(\"NewsWithTINFA\"),     # Count infrastructure news\n        sum(\"NewsWithME\").alias(\"NewsWithME\"),           # Count macro news\n        sum(\"NewsWithPS\").alias(\"NewsWithPS\"),           # Count public sector news\n        sum(\"NewsWithMI\").alias(\"NewsWithMI\"),           # Count incident news\n        sum(\"Total\").alias(\"TotalThemesFindings\"),       # Total theme occurrences\n        count(\"Total\").alias(\"NumberOfNews\")             # Total news count\n    )\n)\n\n# PART 2: Create complete date × location × theme combination grid\n# This ensures we have records for all dates, even with zero news (important for time-series)\nGKG_PORTS_OV_FIRST_APPROACH_CD = (\n    TABLE_OF_DATES  # All dates in our analysis period\n    .crossJoin(  # Cartesian product with all possible combinations\n        GKG_PORTS_OV_FIRST_APPROACH\n        .select(\"Total\", \"LocationCode\")\n        .distinct()\n    )\n)\n\n# PART 3: Left join to fill in missing date/location/theme combinations with zeros\nGKG_PORTS_FIRST_APPROACH = (\n    GKG_PORTS_OV_FIRST_APPROACH_CD\n    .join(GKG_PORTS_OV_FIRST_APPROACH, [\"Date\", \"Total\", \"LocationCode\"], \"left\")\n    .fillna(0)  # Fill missing combinations with 0 (no news that day)\n    .persist()  # Cache for repeated use in visualizations\n)\n\n# Materialize the dataframe to validate row count\nGKG_PORTS_FIRST_APPROACH.count()\n\n# NOTE: Window functions for growth analysis applied in visualization cells below\n# Example: lag(col(\"NumberOfNews\"), 1).over(Window.partitionBy(\"Total\").orderBy(\"Date\"))"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b34c5d-6f8a-4bcd-af4d-f8dd6b5c6cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(GKG_PORTS_FIRST_APPROACH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd98781a-ca92-4e56-ab59-13f2877fac9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "GKG_PRINCIPAL_CLEANING\n",
    ".withColumn(\"BaseNews\", when((col(\"THEMES\").like(\"%PORT%\")) & (col(\"THEMES\").like(\"%TRANSPORT%\")) & (~col(\"THEMES\").like(\"%AIRPORT%\")),1)) # BASE FLAG NEWS THAT HAVE THEME PORT AND TRANSPORTATIONS AND NOT AIRPORTS\n",
    ".filter(\"BaseNews == 1\") # SELECT NEWS ONLY RELATED TO BASE NEWS FLAG\n",
    ".filter(\"LocationCode == 'CA02'\")\n",
    ".filter(\"Date == '2024-07-23'\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a3c658e-e4be-4d9a-aae4-c3b47ea90aaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKR0tHX1BPUlRTX0ZJUlNUX0FQUFJPQUNICi53aXRoQ29sdW1uKCJOdW1iZXJPZk5ld3NQYXN0RGF5IiwgbGFnKGNvbCgiTnVtYmVyT2ZOZXdzIiksMSkub3ZlcihXaW5kb3cucGFydGl0aW9uQnkoIlRvdGFsIiwiTG9jYXRpb25Db2RlIikub3JkZXJCeSgiRGF0ZSIpKSkKLndpdGhDb2x1bW4oIkdyb3d0aE51bWJlck9mTmV3c1Bhc3REYXkiLCAoY29sKCJOdW1iZXJPZk5ld3MiKSAtIGNvbCgiTnVtYmVyT2ZOZXdzUGFzdERheSIpKS9jb2woIk51bWJlck9mTmV3c1Bhc3REYXkiKSkKLmZpbHRlcigiTG9jYXRpb25Db2RlID09ICdDQTAyJyIpCi53aXRoQ29sdW1uKCJMYWdHcm93dGhOdW1iZXJPZk5ld3NQYXN0RGF5IiwgbGFnKGNvbCgiR3Jvd3RoTnVtYmVyT2ZOZXdzUGFzdERheSIpLDEpLm92ZXIoV2luZG93LnBhcnRpdGlvbkJ5KCJUb3RhbCIsIkxvY2F0aW9uQ29kZSIpLm9yZGVyQnkoIkRhdGUiKSkpCi5maWxsbmEoMCkKKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView3ee796d\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView3ee796d\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView3ee796d\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView3ee796d) SELECT `Date`,SUM(`NumberOfNews`) `column_25cd392496` FROM q GROUP BY `Date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView3ee796d\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "Date",
             "id": "column_25cd392413"
            },
            "y": [
             {
              "column": "NumberOfNews",
              "id": "column_25cd392496",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_25cd392417": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392469": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392496": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1f00ed97-4e33-48c4-8e5e-53eeba5b38d0",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.266845703125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Date",
           "type": "column"
          },
          {
           "alias": "column_25cd392496",
           "args": [
            {
             "column": "NumberOfNews",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     }
    }
   ],
   "source": [
    "display(\n",
    "GKG_PORTS_FIRST_APPROACH\n",
    ".withColumn(\"NumberOfNewsPastDay\", lag(col(\"NumberOfNews\"),1).over(Window.partitionBy(\"Total\",\"LocationCode\").orderBy(\"Date\")))\n",
    ".withColumn(\"GrowthNumberOfNewsPastDay\", (col(\"NumberOfNews\") - col(\"NumberOfNewsPastDay\"))/col(\"NumberOfNewsPastDay\"))\n",
    ".filter(\"LocationCode == 'CA02'\")\n",
    ".withColumn(\"LagGrowthNumberOfNewsPastDay\", lag(col(\"GrowthNumberOfNewsPastDay\"),1).over(Window.partitionBy(\"Total\",\"LocationCode\").orderBy(\"Date\")))\n",
    ".fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f7adc9-3fe5-4ae9-ac9e-2c9fd613ff2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "---\n\n## 8. Approach 2: Weighted News Scoring System\n\n### Objective:\nAssign **weighted scores** to news based on the number of critical themes present, emphasizing articles that mention multiple risk factors.\n\n### Hypothesis:\nNews articles mentioning **multiple relevant themes** are more significant indicators of potential disruptions than single-theme articles. A weighted scoring system helps prioritize monitoring efforts.\n\n### Weighting Strategy:\n\nThe more themes an article contains, the higher its importance score:\n\n| Themes Present | Weight Multiplier | Rationale |\n|----------------|-------------------|-----------|\n| **5 themes** | 500× | Extremely rare, likely major incident |\n| **4 themes** | 250× | Significant multi-faceted issue |\n| **3 themes** | 100× | Complex situation affecting multiple areas |\n| **2 themes** | 5× | Moderate concern with multiple factors |\n| **1 theme** | 0× | Excluded (insufficient signal) |\n\n### Why This Approach?\n\n**Advantages:**\n- **Signal amplification**: Multi-theme news gets exponentially higher weight\n- **Noise reduction**: Single-theme news filtered out as potential noise\n- **Prioritization**: Helps identify the most critical news to monitor\n\n**Use Case:**\nLogistics operators can focus attention on high-weighted-score days/locations, indicating complex, multi-factor disruption risks.\n\n### Analysis Method:\n1. Apply same theme identification as Approach 1\n2. Calculate weighted score based on theme count\n3. Aggregate weighted scores by date and location  \n4. Compare with lagged values to detect spikes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a088a332-a509-407d-98be-2a5e0fd0178f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": "# ============================================\n# APPROACH 2: WEIGHTED NEWS SCORING SYSTEM\n# ============================================\n\n# PART 1: Theme identification with weighted scoring\nGKG_PORTS_OV_SECOND_APPROACH = (    \n    GKG_NOT_EMOTIONAL_CHARGE\n    \n    # STEP 1: Apply base filters (same as Approach 1)\n    .withColumn(\"BaseNews\",\n        when(\n            (col(\"THEMES\").like(\"%PORT%\")) &          # Must mention ports\n            (col(\"THEMES\").like(\"%TRANSPORT%\")) &     # Must mention transport\n            (~col(\"THEMES\").like(\"%AIRPORT%\")),       # Exclude airports\n            1\n        )\n    )\n    .filter(\"BaseNews == 1\")\n    \n    # STEP 2: Tag with theme flags\n    .withColumn(\"NewsWithTINFA\", when(col(\"THEMES\").like(\"%TRANSPORT_INFRASTRUCTURE%\"), 1))\n    .withColumn(\"NewsWithTRADE\", when(col(\"THEMES\").like(\"%TRADE%\"), 1))\n    .withColumn(\"NewsWithME\", when(col(\"THEMES\").like(\"%MACROECONOMIC%\"), 1))\n    .withColumn(\"NewsWithPS\", when(col(\"THEMES\").like(\"%PUBLIC_SECTOR%\"), 1))\n    .withColumn(\"NewsWithMI\", when(col(\"THEMES\").like(\"%MARITIME_INCIDENT%\"), 1))\n    .fillna(0)\n    \n    # STEP 3: Count total themes per article\n    .withColumn(\"Total\",\n        col(\"NewsWithTINFA\") + col(\"NewsWithTRADE\") + \n        col(\"NewsWithME\") + col(\"NewsWithPS\") + col(\"NewsWithMI\")\n    )\n    \n    # STEP 4: Filter to negative news only\n    .filter(\"AverageTone < 0\")\n    \n    # STEP 5: Aggregate by date, theme count, and location\n    .groupby(\"Date\", \"Total\", \"LocationCode\")\n    .agg(count(\"Total\").alias(\"NumberOfNews\"))\n    \n    # STEP 6: Apply exponential weighting based on theme count\n    # More themes = exponentially higher importance score\n    .withColumn('WeightedCountOfNews',\n        when(col(\"Total\") == 5, col(\"NumberOfNews\") * 500)   # 5 themes: Critical (500x weight)\n        .when(col('Total') == 4, col(\"NumberOfNews\") * 250)  # 4 themes: Major (250x weight)\n        .when(col(\"Total\") == 3, col(\"NumberOfNews\") * 100)  # 3 themes: Significant (100x weight)\n        .when(col(\"Total\") == 2, col(\"NumberOfNews\") * 5)    # 2 themes: Moderate (5x weight)\n        .otherwise(0)  # 1 theme: Filtered out as noise (0 weight)\n    )\n)\n\n# PART 2: Create complete date × location × theme grid (same as Approach 1)\nGKG_PORTS_OV_SECOND_APPROACH_CD = (\n    TABLE_OF_DATES\n    .crossJoin(\n        GKG_PORTS_OV_SECOND_APPROACH\n        .select(\"Total\", \"LocationCode\")\n        .distinct()\n    )\n)\n\n# PART 3: Join and fill missing combinations\nGKG_PORTS_SECOND_APPROACH = (\n    GKG_PORTS_OV_SECOND_APPROACH_CD\n    .join(GKG_PORTS_OV_SECOND_APPROACH, [\"Date\", \"Total\", \"LocationCode\"], \"left\")\n    .fillna(0)  # Fill missing dates with 0\n    .persist()  # Cache for visualization\n)\n\n# Materialize the dataframe\nGKG_PORTS_SECOND_APPROACH.count()\n\n# NOTE: Visualization cells below aggregate weighted scores by location\n# and calculate day-over-day changes to detect disruption risk spikes"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12de4d4e-944d-4f47-9954-67ffe362694c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKR0tHX1BPUlRTX1NFQ09ORF9BUFBST0FDSAouZ3JvdXBCeSgiRGF0ZSIsIkxvY2F0aW9uQ29kZSIpLmFnZyhzdW0oIldlaWdodGVkQ291bnRPZk5ld3MiKS5hbGlhcygiV2VpZ2h0ZWRDb3VudE9mTmV3cyIpKQoud2l0aENvbHVtbigiTGFnV2VpZ2h0ZWRDb3VudE5ld3MiLCBsYWcoY29sKCJXZWlnaHRlZENvdW50T2ZOZXdzIiksMikub3ZlcihXaW5kb3cucGFydGl0aW9uQnkoIkxvY2F0aW9uQ29kZSIpLm9yZGVyQnkoIkRhdGUiKSkpCi5maWxsbmEoMCkKLmZpbHRlcigoRi5jb2woIkxvY2F0aW9uQ29kZSIpLmlzaW4oCiAgICAgICAgJ0NBMDInLCAgIyBQb3J0IG9mIFZhbmNvdXZlciAtIEJyaXRpc2ggQ29sdW1iaWEKICAgICAgICAnQ0ExMCcsICAjIFBvcnQgb2YgTW9udHJlYWwgLSBRdWViZWMKICAgICAgICAnVVNDQScsICAjIFBvcnQgb2YgTG9zIEFuZ2VsZXMsIExvbmcgQmVhY2gsIE9ha2xhbmQgLSBDYWxpZm9ybmlhCiAgICAgICAgJ0NIMjMnLCAgIyBQb3J0IG9mIFNoYW5naGFpIC0gU2hhbmdoYWkKICAgICAgICAnQ0gzMCcsICAjIFBvcnQgb2YgU2hlbnpoZW4gLSBHdWFuZ2RvbmcgUHJvdmluY2UKICAgICAgICAnQ0gwMicsICAjIFBvcnQgb2YgTmluZ2JvLVpob3VzaGFuIC0gWmhlamlhbmcgUHJvdmluY2UKICAgICAgICAnSkE0MCcsICAjIFBvcnQgb2YgVG9reW8gLSBUb2t5bwogICAgICAgICdKQTE5JywgICMgUG9ydCBvZiBZb2tvaGFtYSAtIEthbmFnYXdhIFByZWZlY3R1cmUKICAgICAgICAnSkEwMScgICAjIFBvcnQgb2YgTmFnb3lhIC0gQWljaGkgUHJlZmVjdHVyZQogICAgKSkpIAop\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView7aeffd6\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView7aeffd6\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView7aeffd6\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7aeffd6) SELECT `Date`,SUM(`WeightedCountOfNews`) `column_25cd392487` FROM q GROUP BY `Date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7aeffd6\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "LocationCode",
             "id": "column_bd7f34f63"
            },
            "x": {
             "column": "Date",
             "id": "column_25cd392499"
            },
            "y": [
             {
              "column": "WeightedCountOfNews",
              "id": "column_25cd392487",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "LagWeightedCountNews": {
             "type": "line",
             "yAxis": 0
            },
            "WeightedCountOfNews": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392445": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392458": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392464": {
             "type": "line",
             "yAxis": 0
            },
            "column_25cd392487": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "796c6951-53c0-48c5-8e71-d63c272e7a85",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 13,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Date",
           "type": "column"
          },
          {
           "column": "LocationCode",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Date",
           "type": "column"
          },
          {
           "alias": "column_25cd392487",
           "args": [
            {
             "column": "WeightedCountOfNews",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "LocationCode",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     }
    }
   ],
   "source": [
    "display(\n",
    "GKG_PORTS_SECOND_APPROACH\n",
    ".groupBy(\"Date\",\"LocationCode\").agg(sum(\"WeightedCountOfNews\").alias(\"WeightedCountOfNews\"))\n",
    ".withColumn(\"LagWeightedCountNews\", lag(col(\"WeightedCountOfNews\"),2).over(Window.partitionBy(\"LocationCode\").orderBy(\"Date\")))\n",
    ".fillna(0)\n",
    ".filter(col(\"LocationCode\").isin(\n",
    "        'CA02',  # Port of Vancouver - British Columbia\n",
    "        'CA10',  # Port of Montreal - Quebec\n",
    "        'USCA',  # Port of Los Angeles, Long Beach, Oakland - California\n",
    "        'CH23',  # Port of Shanghai - Shanghai\n",
    "        'CH30',  # Port of Shenzhen - Guangdong Province\n",
    "        'CH02',  # Port of Ningbo-Zhoushan - Zhejiang Province\n",
    "        'JA40',  # Port of Tokyo - Tokyo\n",
    "        'JA19',  # Port of Yokohama - Kanagawa Prefecture\n",
    "        'JA01'  # Port of Nagoya - Aichi Prefecture\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af6887e-7a2c-4eb2-b793-a05fd9eb59a4",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": "---\n\n## 9. Summary and Key Takeaways\n\n### What We Accomplished:\n\n✅ **Data Quality Assurance**\n- Cleaned and validated port location coordinates\n- Standardized GDELT GKG data formats  \n- Filtered emotionally-charged news to ensure objective analysis\n\n✅ **Advanced Analytics Techniques**\n- **Approach 1**: Time-series growth analysis of theme-specific news\n- **Approach 2**: Weighted scoring system for multi-theme news prioritization\n\n✅ **Predictive Framework**\n- Established baseline metrics for normal news patterns\n- Created lagging indicators to detect anomalous spikes\n- Built foundation for real-time disruption alerting\n\n### Data Quality Best Practices Demonstrated:\n\n1. **Validation at Source**: Filter NULL values before processing\n2. **Standardization**: Convert all coordinates to consistent decimal format\n3. **Bias Removal**: Filter emotionally-charged content for objective analysis\n4. **Completeness**: Use cross-join to ensure all date/location combinations present\n5. **Documentation**: Clear comments explaining every transformation step\n\n### Next Steps:\n\n- 📊 **Visualization**: Create Power BI dashboards for operational monitoring\n- 🤖 **ML Models**: Train predictive models on historical patterns\n- ⚠️ **Alerting**: Implement real-time alerts when thresholds exceeded\n- 📈 **Validation**: Back-test against known disruption events\n\n---\n\n**End of Notebook**"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8463b377-dde0-4e47-a801-2417a98d7b57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Country Exploration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}